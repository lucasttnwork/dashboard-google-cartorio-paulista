# ğŸ›ï¸ Dashboard Google - CartÃ³rio Paulista

Sistema completo de monitoramento e anÃ¡lise de avaliaÃ§Ãµes do Google Business Profile para o CartÃ³rio Paulista, com coleta automÃ¡tica e dashboard analÃ­tico em tempo real.

## ğŸ“‹ VisÃ£o Geral

Este projeto consiste em uma plataforma completa que automatiza a coleta de avaliaÃ§Ãµes do Google Business Profile e apresenta insights analÃ­ticos atravÃ©s de um dashboard moderno e responsivo.

### ğŸ¯ Funcionalidades Principais

- **ğŸ¤– Coleta AutomÃ¡tica**: Scraper Playwright que executa a cada hora
- **ğŸ“Š Dashboard AnalÃ­tico**: Interface moderna com grÃ¡ficos e KPIs
- **ğŸ‘¥ AnÃ¡lise de Colaboradores**: IdentificaÃ§Ã£o automÃ¡tica de menÃ§Ãµes em avaliaÃ§Ãµes
- **ğŸ“ˆ TendÃªncias Temporais**: AnÃ¡lise de evoluÃ§Ã£o das avaliaÃ§Ãµes ao longo do tempo
- **ğŸ’¾ Armazenamento Seguro**: Banco PostgreSQL com RLS via Supabase
- **ğŸ¥ Monitoramento**: Health checks e mÃ©tricas em tempo real
- **ğŸ³ Deploy Containerizado**: Docker + Railway para produÃ§Ã£o

## ğŸ—ï¸ Arquitetura

```
Dashboard Google - CartÃ³rio Paulista/
â”œâ”€â”€ dashboard-frontend/          # Next.js 15 + TypeScript + Tailwind
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/                # App Router (Next.js 15)
â”‚   â”‚   â”œâ”€â”€ components/         # Componentes reutilizÃ¡veis
â”‚   â”‚   â””â”€â”€ lib/               # Hooks, adapters, utils
â”‚   â””â”€â”€ Dockerfile             # Container para produÃ§Ã£o
â”‚
â”œâ”€â”€ scraper/                   # Worker de coleta automÃ¡tica
â”‚   â”œâ”€â”€ gbp/                  # Scraping do Google Business Profile
â”‚   â”œâ”€â”€ processors/           # Processamento e validaÃ§Ã£o
â”‚   â”œâ”€â”€ storage/             # IntegraÃ§Ã£o Supabase
â”‚   â”œâ”€â”€ scheduler/           # Cron jobs
â”‚   â”œâ”€â”€ monitoring/          # Dashboard de monitoramento
â”‚   â””â”€â”€ Dockerfile          # Container para produÃ§Ã£o
â”‚
â”œâ”€â”€ supabase/               # Schema e configuraÃ§Ãµes do banco
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â””â”€â”€ init.sql       # Schema completo + RPCs + dados exemplo
â”‚   â””â”€â”€ functions/         # Edge Functions (se necessÃ¡rio)
â”‚
â”œâ”€â”€ .github/workflows/     # CI/CD com GitHub Actions
â””â”€â”€ docker-compose.yml    # Desenvolvimento local
```

## ğŸš€ Quick Start

### 1. PrÃ©-requisitos

- **Node.js 18+**
- **Conta Supabase** (para banco PostgreSQL)
- **Conta Railway** (para deploy em produÃ§Ã£o)
- **Google Business Profile** configurado

### 2. ConfiguraÃ§Ã£o do Banco de Dados

1. **Criar projeto no Supabase**
2. **Aplicar schema**:
   ```sql
   -- Execute o conteÃºdo de supabase/sql/init.sql no SQL Editor
   ```
3. **Obter credenciais**:
   - URL do projeto
   - Anon key (para frontend)
   - Service role key (para scraper)

### 3. Setup Local - Frontend

```bash
cd dashboard-frontend
npm install
cp .env.local.example .env.local
# Editar .env.local com suas credenciais Supabase
npm run dev
```

Acesse: http://localhost:3000

### 4. Setup Local - Scraper

```bash
cd scraper
npm install
npm run install-browsers  # Instala Chromium para Playwright
cp .env.example .env
# Editar .env com suas credenciais
npm start
```

Dashboard de monitoramento: http://localhost:3001

### 5. Docker (Recomendado)

```bash
# Copiar .env.example para .env e configurar
cp .env.example .env

# Subir todos os serviÃ§os
docker-compose up -d

# Ver logs
docker-compose logs -f
```

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

#### Frontend (.env.local)
```env
NEXT_PUBLIC_SUPABASE_URL=https://seu-projeto.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=sua-anon-key
```

#### Scraper (.env)
```env
SUPABASE_URL=https://seu-projeto.supabase.co
SUPABASE_SERVICE_ROLE_KEY=sua-service-role-key
GBP_SEARCH_URL=https://www.google.com/maps/place/CartÃ³rio+Paulista
CRON_SCHEDULE=0 */1 * * *  # A cada hora
PORT=3001
LOG_LEVEL=info
```

### ConfiguraÃ§Ã£o do Google Business Profile

1. **Encontrar URL do Google Maps** da sua empresa
2. **Configurar GBP_SEARCH_URL** no scraper
3. **Testar scraping**: `cd scraper && npm test`

## ğŸš€ Deploy em ProduÃ§Ã£o

### Railway (Recomendado)

1. **Conectar repositÃ³rio** ao Railway
2. **Criar dois serviÃ§os**:
   - **Frontend**: Detecta automaticamente `dashboard-frontend/`
   - **Scraper**: Detecta automaticamente `scraper/`

3. **Configurar variÃ¡veis de ambiente** em cada serviÃ§o
4. **Deploy automÃ¡tico** via GitHub Actions

### Manual com Docker

```bash
# Build das imagens
docker build -t cartorio-frontend ./dashboard-frontend
docker build -t cartorio-scraper ./scraper

# Deploy com docker-compose
docker-compose -f docker-compose.prod.yml up -d
```

## ğŸ“Š Monitoramento

### Frontend
- **Health Check**: `GET /api/health`
- **Dashboard**: Interface principal com KPIs e grÃ¡ficos

### Scraper
- **Dashboard**: http://localhost:3001
- **Health Check**: `GET /health`
- **Status API**: `GET /api/status`
- **MÃ©tricas**: `GET /api/metrics`
- **Trigger Manual**: `POST /api/trigger`

### Logs
- **Frontend**: Next.js logs
- **Scraper**: `logs/scraper.log`, `logs/error.log`

## ğŸ”„ CI/CD

O projeto inclui pipelines automatizados com GitHub Actions:

- **Lint e Build** em PRs e pushes
- **Testes de SeguranÃ§a** com npm audit
- **Deploy AutomÃ¡tico** para Railway no branch `main`
- **Health Checks** pÃ³s-deploy

### Secrets NecessÃ¡rios

```
RAILWAY_TOKEN=seu-token-railway
NEXT_PUBLIC_SUPABASE_URL=url-supabase
NEXT_PUBLIC_SUPABASE_ANON_KEY=anon-key
SUPABASE_URL=url-supabase
SUPABASE_SERVICE_ROLE_KEY=service-role-key
GBP_SEARCH_URL=url-google-maps
FRONTEND_URL=https://seu-frontend.railway.app
SCRAPER_URL=https://seu-scraper.railway.app
```

## ğŸ“ˆ Funcionalidades Implementadas

### âœ… Dashboard Frontend
- **KPIs em Tempo Real**: Total de avaliaÃ§Ãµes, rating mÃ©dio, % 5 estrelas
- **GrÃ¡ficos Interativos**: EvoluÃ§Ã£o temporal com Recharts
- **Tabela de AvaliaÃ§Ãµes**: Filtros avanÃ§ados, busca, export CSV
- **Ranking de Colaboradores**: MenÃ§Ãµes automÃ¡ticas em avaliaÃ§Ãµes
- **Design Responsivo**: Mobile-first com Tailwind CSS
- **Temas**: Light/Dark mode automÃ¡tico

### âœ… Sistema de Scraping
- **Coleta AutomÃ¡tica**: Cron job configurÃ¡vel (padrÃ£o: hora em hora)
- **Navegador Headless**: Playwright com Chromium
- **Processamento Inteligente**: DeduplicaÃ§Ã£o SHA-256
- **AnÃ¡lise NLP**: DetecÃ§Ã£o automÃ¡tica de menÃ§Ãµes a colaboradores
- **Armazenamento Seguro**: PostgreSQL com RLS

### âœ… Banco de Dados
- **Schema Completo**: Tabelas otimizadas com Ã­ndices
- **RPCs Personalizadas**: FunÃ§Ãµes para estatÃ­sticas e tendÃªncias
- **Row Level Security**: PolÃ­ticas de acesso configuradas
- **Dados de Exemplo**: 10 reviews e 9 colaboradores para teste

### âœ… Infraestrutura
- **ContainerizaÃ§Ã£o**: Dockerfiles otimizados para produÃ§Ã£o
- **Health Checks**: Monitoramento de saÃºde dos serviÃ§os
- **Logging Estruturado**: Winston com nÃ­veis configurÃ¡veis
- **Graceful Shutdown**: Limpeza adequada de recursos

## ğŸ› ï¸ Desenvolvimento

### Estrutura de Dados

#### Reviews
```sql
CREATE TABLE reviews (
  review_id TEXT PRIMARY KEY,
  location_id TEXT,
  rating INTEGER CHECK (rating BETWEEN 1 AND 5),
  comment TEXT,
  reviewer_name TEXT,
  create_time TIMESTAMPTZ,
  -- ... outros campos
);
```

#### Colaboradores
```sql
CREATE TABLE collaborators (
  id BIGSERIAL PRIMARY KEY,
  full_name TEXT NOT NULL,
  department TEXT,
  is_active BOOLEAN DEFAULT TRUE,
  aliases TEXT[] DEFAULT '{}'::TEXT[]
);
```

### APIs Principais

#### Frontend
- `GET /api/health` - Health check

#### Scraper
- `GET /health` - Health check
- `GET /api/status` - Status do scraper
- `GET /api/metrics` - MÃ©tricas detalhadas
- `POST /api/trigger` - Executar scraping manual
- `POST /api/scheduler/pause` - Pausar agendamento
- `POST /api/scheduler/resume` - Retomar agendamento

### Comandos Ãšteis

```bash
# Frontend
cd dashboard-frontend
npm run dev          # Desenvolvimento
npm run build        # Build produÃ§Ã£o
npm run lint         # Lint cÃ³digo

# Scraper
cd scraper
npm start           # Iniciar aplicaÃ§Ã£o
npm test            # Teste de scraping
node index.js health # Check saÃºde do sistema
node index.js config # Ver configuraÃ§Ã£o atual

# Docker
docker-compose up -d              # Subir serviÃ§os
docker-compose logs -f scraper    # Ver logs do scraper
docker-compose down               # Parar serviÃ§os
```

## ğŸ› Troubleshooting

### Problemas Comuns

1. **Scraper nÃ£o encontra reviews**:
   - Verificar se `GBP_SEARCH_URL` estÃ¡ correto
   - Testar URL manualmente no navegador
   - Verificar logs: `docker-compose logs scraper`

2. **Frontend mostra dados mock**:
   - Verificar variÃ¡veis `NEXT_PUBLIC_SUPABASE_*`
   - Confirmar se RPCs foram aplicadas no Supabase
   - Ver console do navegador para erros

3. **Erro de conexÃ£o com banco**:
   - Verificar `SUPABASE_URL` e `SUPABASE_SERVICE_ROLE_KEY`
   - Testar: `cd scraper && node index.js health`
   - Confirmar se IP estÃ¡ na whitelist do Supabase

4. **Deploy Railway falha**:
   - Verificar se variÃ¡veis de ambiente estÃ£o configuradas
   - Conferir logs de build no Railway
   - Verificar health checks apÃ³s deploy

### Logs e Debug

```bash
# Logs detalhados do scraper
LOG_LEVEL=debug node index.js start

# Logs do Docker
docker-compose logs -f --tail=100 scraper

# Health check manual
curl http://localhost:3001/health
curl http://localhost:3000/api/health
```

## ğŸ¤ ContribuiÃ§Ã£o

1. **Fork** o projeto
2. **Crie** uma branch: `git checkout -b feature/nova-funcionalidade`
3. **Commit** suas mudanÃ§as: `git commit -m 'Adiciona nova funcionalidade'`
4. **Push** para a branch: `git push origin feature/nova-funcionalidade`
5. **Abra** um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ†˜ Suporte

- **Issues**: Use o GitHub Issues para reportar bugs
- **DocumentaÃ§Ã£o**: Consulte os READMEs especÃ­ficos em cada diretÃ³rio
- **Logs**: Sempre inclua logs relevantes ao reportar problemas

---

**ğŸ›ï¸ CartÃ³rio Paulista** - Sistema de Monitoramento de AvaliaÃ§Ãµes
Desenvolvido com â¤ï¸ usando Next.js, Playwright, Supabase e Railway.